{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3f37b2ae-ab59-49bd-9613-b53548e0b0a3",
   "metadata": {},
   "source": [
    "pyspark.Accumulator\n",
    "\n",
    "A shared variable that can be accumulated, i.e., has a commutative and associative “add” operation. Worker tasks on a Spark cluster can add values to an Accumulator with the += operator, but only the driver program is allowed to access its value, using value. Updates from the workers get propagated automatically to the driver program."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2b0138c-d5a0-4d5b-8489-0de3d365777f",
   "metadata": {},
   "source": [
    "Accumulators\n",
    "\n",
    "Accumulators provide a way of aggregating values in worker nodes and sending back the final value to the driver program\n",
    "Used to count events that occur during job execution and debugging purpose\n",
    "Used to implement counters similar to MapReduce counters.\n",
    "Limitations:\n",
    "\n",
    "If the executor failed for any reason, you would see an inconsistent value for count or sum as it executes from the beginning again.\n",
    "\n",
    "Requirement: Count the empty lines in a data set distributed across multiple worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b630387-80cd-4ef9-8c1e-24c54ac29364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/06 18:22:20 WARN Utils: Your hostname, ashish-Lenovo-ideapad-130-15IKB resolves to a loopback address: 127.0.1.1; using 192.168.1.108 instead (on interface wlp2s0)\n",
      "23/02/06 18:22:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/06 18:22:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb7b53f4-c2c3-4cb8-80f3-4c8ade7cc036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Lines: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#create the RDD\n",
    "TelecomRDD = sc.textFile(\"../in/TelecomDataAccumulators.csv\");\n",
    "#Create and initialize the accumulator to count blank lines in  the file.\n",
    "blankLinesCounter = sc.accumulator(0)\n",
    "#verify each line and increment the counter\n",
    "TelecomRDD.foreach(lambda line: blankLinesCounter.add(1) if len(line) == 0 else None)\n",
    "print(\"Empty Lines: \" + str(blankLinesCounter.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55a04180-df4c-4d05-9497-6eaa5c3bbcdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TelecomRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398a0a3a-0e99-4db5-80cb-1bb85ff50f9a",
   "metadata": {},
   "source": [
    "# Attempt to read the file with blank lines as SQL DataFrame and PySpark.Pandas DataFrame to log the errors (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e412583-1c8c-4806-80b6-e8218b48f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb4f4da0-4483-4d64-8d67-723f81bef3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", False).csv('../in/TelecomDataAccumulators.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ca75010-17e0-416c-abef-a8740574f7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0='TXCUST00001', _c1='982120000', _c2='Male', _c3='N', _c4='PrePaid', _c5='Active', _c6='Active', _c7='InActive', _c8='20', _c9='N')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dae127fd-00f3-4a6f-b835-7596e5bf8954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f713cc0b-c77f-4438-bf0d-f02243222e39",
   "metadata": {},
   "source": [
    "## Read operations to get SQL DataFrame or PySpark.Pandas DataFrames by default remove blank lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a2ccc78-8874-42ca-9bc9-5831da065d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import pandas as ppd\n",
    "df_pandas = ppd.read_csv('../in/TelecomDataAccumulators.csv', \n",
    "                         names = ['CustomerID', 'MobileNumber', 'Gender', 'SeniorCitizen', 'Mode', 'Calls', 'SMS', 'InternetServiceStatus', 'MonthlyCharges', 'CustomerChurn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45233e73-d22a-41d8-a5e4-b875facf7e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba29d7f7-39e8-49ca-951b-e8e5bf104452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
