{
 "cells": [
  {
   "cell_type": "raw",
   "id": "45d55a2e-95b1-4ec3-8ab5-3ac1cd16d605",
   "metadata": {},
   "source": [
    "Requirement 2:\n",
    "\n",
    "Compute revenue generated by all postpaid and prepaid service customers.\n",
    "\n",
    "Schema details:\n",
    "CustomerID, Mobile Number, Gender, SeniorCitizen Flag, Mode, Calls, SMS, Internet Service Status, MonthlyCharges (USD), CustomerChurn Flag\n",
    "\n",
    "Before solving the requirement, let us understand the concept of Paired RDD.\n",
    "\n",
    "In most analytical programs, data in the <Key, Value> format provides a feasible way to perform computations. RDD with this data format is called Paired RDD. Every record contains only two fields, key and value.\n",
    "\n",
    "Solution:\n",
    "\n",
    "Step 1: Create a paired RDD with the <key, value> structure. In this requirement, it is <Mode, MonthlyCharges>.\n",
    "\n",
    "Step 2: Group all the fields and apply the sum arithmetic function\n",
    "\n",
    "Step 3: Use the same paired RDD we created and apply the reduceByKey() transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a08a89-afc3-448e-b943-ec1257257683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/02 15:05:38 WARN Utils: Your hostname, ashish-Lenovo-ideapad-130-15IKB resolves to a loopback address: 127.0.1.1; using 192.168.1.187 instead (on interface wlp2s0)\n",
      "23/02/02 15:05:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/02 15:05:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e94e57-616f-46b2-87eb-3c44b3c74b40",
   "metadata": {},
   "source": [
    "# 1: Using RDD Transformations"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0966b804-5e71-490f-894d-4b00251d0b3c",
   "metadata": {},
   "source": [
    "pyspark.RDD.groupByKey\n",
    "\n",
    "Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with numPartitions partitions.\n",
    "\n",
    "Notes\n",
    "\n",
    "If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will provide much better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41214f67-e6af-4c79-8456-783df0e93f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "\n",
    "sorted(rdd.groupByKey().mapValues(len).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5938397c-1796-42d8-addd-79227bfd255c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [1, 1]), ('b', [1])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(rdd.groupByKey().mapValues(list).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f902b06-1f55-47df-9a44-96768be78bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Telecomdata = sc.textFile(\"../in/TelecomData.csv\")\n",
    "PairedData = Telecomdata.map(lambda record:(record.split(\",\")[4],int(record.split(\",\")[8])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a551d30-1f34-4fe0-8130-7cec43081e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply groupByKey() transformation on paired RDD\n",
    "grpRecords = PairedData.groupByKey()\n",
    "#Use map transformation to sum the grouped values of the same key.\n",
    "aggregatedData = grpRecords.map(lambda x:(x[0],sum(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cd3a9e9-4cc2-4a01-a60b-9a58eefdf23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PostPaid', 8638), ('PrePaid', 10364)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregatedData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b6c23-9be8-4e06-a19a-a5f6a979fc56",
   "metadata": {},
   "source": [
    "## Methods used above: map() and groupByKey()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59a663d0-89f1-44d4-886f-fab0f681f595",
   "metadata": {},
   "source": [
    "map()\n",
    "Narrow transformation used to create a new RDD from the parent RDD with the required structure and fields\n",
    "Allows fetching required fields from parent RDD\n",
    "Used to create Paired RDDs\n",
    "\n",
    "groupByKey()\n",
    "\n",
    "A wide transformation used to group values of the same key. It should be used only on Paired RDDs.\n",
    "Results into new RDD with the data format as <Key, List of values>\n",
    "Note: The above solution map() transformation is used to add all grouped values. The sum is the Python function to perform the addition operation.\n",
    "\n",
    "groupByKey() operation performs huge amounts of data shuffling, leading to network congestion and performance issues."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bfc20164-60ca-4639-8413-3dc3228a3fe4",
   "metadata": {},
   "source": [
    "Let us solve the requirement using an alternate solution with reduceByKey()\n",
    "\n",
    "reduceByKey()\n",
    "\n",
    "Wide transformation performs aggregation operations on the same key values. It should be used only on Paired RDDs.\n",
    "Results into new RDD with the data format as <Key, aggregated value>\n",
    "Launches combiners to perform local aggregations before final reducing. The amount of data shuffling reduces drastically and improves the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02f03140-1b72-4d58-8d45-77c6ec237cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregatedData = PairedData.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b7f9930-3e80-42ee-a620-3f41c62a698d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PostPaid', 8638), ('PrePaid', 10364)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregatedData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ba50e4-80ab-46bc-ad7c-aab8739777ba",
   "metadata": {},
   "source": [
    "# 2: Using Spark SQL DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "483f4f12-a6a9-4f49-8607-e1e29ea4aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bf81f59-1821-446f-b7c5-c0d4745da8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", False).csv('../in/TelecomData.csv')\n",
    "\n",
    "df = df.withColumnRenamed(\"_c0\", \"CustomerID\") \\\n",
    ".withColumnRenamed(\"_c1\", \"Mobile Number\") \\\n",
    ".withColumnRenamed(\"_c2\", \"Gender\") \\\n",
    ".withColumnRenamed(\"_c3\", \"SeniorCitizen\") \\\n",
    ".withColumnRenamed(\"_c4\", \"Mode\") \\\n",
    ".withColumnRenamed(\"_c5\", \"Calls\") \\\n",
    ".withColumnRenamed(\"_c6\", \"SMS\") \\\n",
    ".withColumnRenamed(\"_c7\", \"InternetServiceStatus\") \\\n",
    ".withColumnRenamed(\"_c8\", \"MonthlyCharges\") \\\n",
    ".withColumnRenamed(\"_c9\", \"CustomerChurn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "045e01c9-55c4-4cc0-9835-1c4b8a3d48ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------+-------------+--------+------+------+---------------------+--------------+-------------+\n",
      "| CustomerID|Mobile Number|Gender|SeniorCitizen|    Mode| Calls|   SMS|InternetServiceStatus|MonthlyCharges|CustomerChurn|\n",
      "+-----------+-------------+------+-------------+--------+------+------+---------------------+--------------+-------------+\n",
      "|TXCUST00001|    982120000|  Male|            N| PrePaid|Active|Active|             InActive|            20|            N|\n",
      "|TXCUST00002|    982120001|  Male|            N|PostPaid|Active|Active|             InActive|            25|            N|\n",
      "|TXCUST00003|    982120002|  Male|            N| PrePaid|Active|Active|             InActive|            20|            Y|\n",
      "|TXCUST00004|    982120003|  Male|            Y| PrePaid|Active|Active|             InActive|            25|            N|\n",
      "|TXCUST00005|    982120004|  Male|            N| PrePaid|Active|Active|             InActive|            15|            N|\n",
      "+-----------+-------------+------+-------------+--------+------+------+---------------------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35b0f4d8-113f-4c19-a09a-a06d457a8eca",
   "metadata": {},
   "source": [
    "pyspark.sql.DataFrame.groupBy\n",
    "\n",
    "Groups the DataFrame using the specified columns, so we can run aggregation on them. See GroupedData for all the available aggregate functions.\n",
    "\n",
    "groupby() is an alias for groupBy().\n",
    "\n",
    "Ex:\n",
    "\n",
    "df.groupBy().avg().collect()\n",
    "[Row(avg(age)=3.5)]\n",
    "\n",
    "sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\n",
    "[Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
    "\n",
    "sorted(df.groupBy(df.name).avg().collect())\n",
    "[Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
    "\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-groupby.html\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.groupBy.html"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9e3f648-0dfb-4ddf-bf92-6a119c2c10dc",
   "metadata": {},
   "source": [
    "\"MonthlyCharges\" column as read from the text file is read as String (on which we cannot perform aggregation).\n",
    "\n",
    "df.groupBy('Mode').sum('MonthlyCharges').collect()\n",
    "\n",
    "AnalysisException: \"MonthlyCharges\" is not a numeric column. Aggregation function can only be applied on a numeric column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cfebb62-f2c5-4167-8d71-807b7307aac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df = df.withColumn(\"MonthlyCharges\", col(\"MonthlyCharges\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f82ac63-f7e1-46fa-856a-7846261c3022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|    Mode|sum(MonthlyCharges)|\n",
      "+--------+-------------------+\n",
      "|PostPaid|               8638|\n",
      "| PrePaid|              10364|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Mode').sum('MonthlyCharges').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1316f0a7-6182-4377-a0d1-d080cddfee84",
   "metadata": {},
   "source": [
    "# 3: Using PySpark Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "790c695b-bb07-4d2e-b55b-85ef3dfdbe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "/home/ashish/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import pandas as ppd\n",
    "df_pandas = ppd.read_csv('../in/TelecomData.csv', \n",
    "                         names = ['CustomerID', 'MobileNumber', 'Gender', 'SeniorCitizen', 'Mode', 'Calls', 'SMS', 'InternetServiceStatus', 'MonthlyCharges', 'CustomerChurn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27cfa577-b4fd-4112-8e68-297450ce88d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mode\n",
       "PostPaid     8638\n",
       "PrePaid     10364\n",
       "Name: MonthlyCharges, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.groupby('Mode')['MonthlyCharges'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396bfb16-86d3-4a3a-ad6b-1326ddc0fcde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
